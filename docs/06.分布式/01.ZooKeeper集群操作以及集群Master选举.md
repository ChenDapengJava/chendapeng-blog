---
title: ZooKeeper集群操作以及集群Master选举
date: 2020-09-15 12:45:11
permalink: /pages/848b43/
categories: 
  - 分布式
  - ZooKeeper
tags: 
  - ZooKeeper
---

作者：行百里er

博客：[https://chendapeng.cn](https://chendapeng.cn)
:::tip
这里是 行百里er 的博客：行百里者半九十，凡事善始善终，吾将上下而求索！
:::


## ZooKeeper介绍

`ZooKeeper` 是一个为 **分布式应用** 提供的 **分布式** 、开源的 **协调服务** 。

它公开了一组简单的 `原语` ，分布式应用程序可以根据这些原语来实现用于 **同步** 、**配置维护** 以及 **命名** 的更高级别的服务。

怎么理解协调服务呢？比如我们有很多应用程序，他们之间都需要读写维护一个 id ，那么这些 id 怎么命名呢，程序一多，必然会乱套，`ZooKeeper` 能协调这些服务，解决命名、配置、同步等问题，而做到这些，只需要一组简单的 `原语` 即可：

```
create : 在树中的某个位置创建一个节点

delete : 删除一个节点

exists : 测试节点是否存在于某个位置

get data : 从节点读取数据

set data : 往一个节点里写入数据

get children : 检索节点的子节点列表

sync : 等待数据被传播
```

从这些 `ZooKeeper` （以下简称ZK）的 API 可以看到，都是围绕 **Node** 来操作，下文实操看一下怎么操作 **Node** 。

### ZooKeeper 的特征

- 简单

`ZooKeeper` 允许分布式进程通过 **共享的层级命名空间** 相互协调，该命名空间的组织类似于标准文件系统。

命名空间由 `数据寄存器` 组成，在 `ZooKeeper` 称为 **znodes** ，它们类似于文件和目录。

与典型的文件系统不同，它是为 **存储** 而设计的，`ZooKeeper` 数据保存在 **内存** 中，这意味着`ZooKeeper` 可以实现 **高吞吐量** 和 **低延迟数** 。

`ZooKeeper` 很重视 **高性能**，**高可用性** ，**严格有序访问** ：性能高意味着它可以在大型分布式系统中使用；而他又具备可靠性，这使它不会成为单点故障；严格的排序意味着可以在客户端上实现复杂的同步原语。

- 可被复制（高可用）

像它协调的分布式进程一样，`ZooKeeper` 本身也可以在称为集合的一组主机上进行复制。

组成`ZooKeeper` 服务的服务器都必须彼此了解。它们维护内存中的状态镜像，以及持久存储中的事务日志和快照。只要大多数服务器可用，`ZooKeeper` 服务将可用。

客户端连接到单个 `ZooKeeper` 服务器。客户端维护一个 `TCP连接` ，通过该连接发送请求，获取响应，获取监视事件并发送心跳。如果与服务器的 `TCP连接` 断开，则客户端将连接到其他服务器。


![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ca1e77157a6d483c908729285340667c~tplv-k3u1fbpfcp-watermark.image?)

- 有序的

`ZooKeeper` 用一个反映所有 `ZooKeeper` **事务顺序** 的数字标记每个更新。后续操作可以使用该命令来实现更高级别的抽象，例如 **同步** 、 **分布式锁** 。

- 快

在 **读取为主** 的工作负载中，它特别快。

`ZooKeeper` 应用程序可在数千台计算机上运行，并且在读取比写入更常见的情况下，其性能最佳，比率约为10：1。

### 分层命名空间

ZooKeeper提供的名称空间与标准文件系统的名称空间非常相似。

名称是由 **斜杠** （`/`）分隔的一系列路径元素。`ZooKeeper` 命名空间中的每个节点均由路径标识。


![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ba318732dcc14c2485da38f36dcf4098~tplv-k3u1fbpfcp-watermark.image?)

一个 ZK Node 可以存储 **1M** 数据，Node分为 **持久节点** 和 **临时性节点** 。

- 持久节点

与标准文件系统不同，`ZooKeeper` 命名空间中的每个节点都可以具有与其关联的 **数据** 以及 **子节点** 。就像拥有一个文件系统一样，该文件系统也允许文件成为目录。

> ZooKeeper旨在存储协调数据：状态信息，配置，位置信息等，因此存储在每个节点上的数据通常很小，在字节到千字节范围内。

`Znodes` 维护一个统计信息结构，其中包括用于 **数据更改** ，**ACL更改（权限控制）** 和 **时间戳的版本号** ，以允许进行 `缓存验证` 和 `协调更新` 。

`Znode` 的数据每次更改时，版本号都会增加。例如，每当客户端检索数据时，它也会接收数据的版本。

原子地读取和写入存储在名称空间中每个 `Znode` 上的数据。读取将获取与znode关联的所有数据字节，而写入将替换所有数据。每个节点都有一个访问控制列表（ACL），用于限制谁可以做什么。

- 临时节点

只要创建 `Znode` 的会话处于 **活动状态** ，这些 `Znode` 就一致存在。会话结束时，将删除 `Znode` ，这就是临时节点。

类比于web容器比如tomcat的session，创建临时节点的session存在，则node存在，session结束，删除node。

以上是理论知识，还是实际操作一遍比较靠谱，理解一下zk创建连接、node、session这些概念，以及看看zk集群的leader出故障后，选出leader的速度。

## 搭建ZK集群

首先准备 4 台 CentOS 7 虚拟机，都安装好了JDK 8（JDK版本最好不要小于8）。


![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0a27ee44419a49ac91d789fb6c72fa19~tplv-k3u1fbpfcp-watermark.image?)

这四台虚拟机主机名称分别设置为：`zknode01` 、`zknode02` 、`zknode03` 、`zknode04` 。

```sh
hostnamectl set-hostname zknode01
```

主机名在配置 `ZooKeeper` 集群的时候有用。

主机名称配置好之后，还需要配置主机名和IP地址的映射关系，每台主机均编辑 `/etc/hosts` 文件，末尾添加如下内容：

```sh
192.168.242.11 zknode01
192.168.242.12 zknode02
192.168.242.13 zknode03
192.168.242.14 zknode04
```

保证每台主机都能互相 **ping** 通：


![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/52ce095f4e6d4f188a89182cff61341c~tplv-k3u1fbpfcp-watermark.image?)

接下来，先安装配置好其中一台服务器的 `ZooKeeper` ，然后用 `scp` 分发到各个服务器，再分别修改 `zk server` 的 **id** ，这样不用每台虚拟机都执行一遍相同的操作。

> 下载zk，注意一定要是`apache-zookeeper-3.7.1-bin.tar.gz`这个带`bin`的，否则如果不是带bin的，启动的时候会报如下错误：
>
> ```
> Error: Could not find or load main class org.apache.zookeeper.server.quorum.QuorumPeerMain
> Caused by: java.lang.ClassNotFoundException: org.apache.zookeeper.server.quorum.QuorumPeerMain
> ```

保姆式安装zk步骤：

```
将下载好的 apache-zookeeper-3.7.1-bin.tar.gz 放到/opt目录下
1. cd /opt
2. tar xf apache-zookeeper-3.7.1-bin.tar.gz
3. mv apache-zookeeper-3.7.1-bin zookeeper
4. vi /etc/profile

export JAVA_HOME=/usr/local/java
export ZK_HOME=/opt/zookeeper
export PATH=$PATH:$JAVA_HOME/bin:$ZK_HOME/bin

5. source /etc/profile
6. cd /opt/zookeeper/conf
7. cp zoo_sample.cfg zoo.cfg
8. vi zoo.cfg

设置 dataDir=/var/zookeeper

末尾添加：
server.1=zknode01:2888:3888
server.2=zknode02:2888:3888
server.3=zknode03:2888:3888
server.4=zknode04:2888:3888

9. mkdir -p /var/zookeeper
10. echo 1 > /var/zookeeper/myid
```

这样 `zknode01` 的 zkserver 就搭建好了，现在将 **ZooKeeper目录** 和 **配置文件** 分发到其余三台服务器：

```sh
# 传到 zknode02
scp -r /opt/zookeeper/ root@zknode02:/opt/
scp /etc/profile root@zknode02:/etc

# 传到 zknode03
scp -r /opt/zookeeper/ root@zknode03:/opt/
scp /etc/profile root@zknode03:/etc

# 传到 zknode04
scp -r /opt/zookeeper/ root@zknode04:/opt/
scp /etc/profile root@zknode04:/etc
```


**别忘了** ，每台主机都需要执行 `source /etc/profile` 和创建 `/var/zookeeper/myid` 文件，**myid** 的内容分别为 2,3,4 。

这样 **zk集群** 就搭建好了。

## 启动zk集群
按顺序启动 `zknode01` ，`zknode02` ，`zknode03` ，`zknode04` 的zk服务：


```
zkServer.sh start-foreground
```
zk默认后台启动， `start-foreground` 表示前台启动，方便看日志。

启动zknode01的zk server：


![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/008b0551cf1d462c877843adb4a36951~tplv-k3u1fbpfcp-watermark.image?)

会报错，因为 `zoo.cfg` 配置了4台主机，其余三台还未启动，接着启动 `zknode02` 的：


![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/45c0468e56dc4aea981150669c166084~tplv-k3u1fbpfcp-watermark.image?)

现象同 `zknode01` ，继续启动第三台：


![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0fd380aaefbc4e46ae473f5c5c3771f9~tplv-k3u1fbpfcp-watermark.image?)

这个时候也会报 `zknode04` 连接不上（因为还没启动），但是整个 **zk集群** 已经启动了，并且选择了 `zknode03` 这个为leader。

把 `zknode04` 也启动一下：


![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0ca41d4547db4d63b146d6904d215d83~tplv-k3u1fbpfcp-watermark.image?)

启动完成后，开一个 `zknode01` 的zk客户端：

```sh
zkCli.sh

[zk: localhost:2181(CONNECTED) 0] help
ZooKeeper -server host:port -client-configuration properties-file cmd args
	addWatch [-m mode] path # optional mode is one of [PERSISTENT, PERSISTENT_RECURSIVE] - default is PERSISTENT_RECURSIVE
	addauth scheme auth
	close 
	config [-c] [-w] [-s]
	connect host:port
	create [-s] [-e] [-c] [-t ttl] path [data] [acl]
	delete [-v version] path
	deleteall path [-b batch size]
	delquota [-n|-b] path
	get [-s] [-w] path
	getAcl [-s] path
	getAllChildrenNumber path
	getEphemerals path
	history 
	listquota path
	ls [-s] [-w] [-R] path
	printwatches on|off
	quit 
```

用上面的命令操作一波：

```sh
[zk: localhost:2181(CONNECTED) 1] ls
ls [-s] [-w] [-R] path
[zk: localhost:2181(CONNECTED) 2] ls /
[zookeeper]
[zk: localhost:2181(CONNECTED) 3] 
[zk: localhost:2181(CONNECTED) 3] create /laogong
Created /laogong
[zk: localhost:2181(CONNECTED) 4] ls /
[laogong, zookeeper]
[zk: localhost:2181(CONNECTED) 5] get /laogong 
null
[zk: localhost:2181(CONNECTED) 6] create /laogong "laogong"
Node already exists: /laogong
[zk: localhost:2181(CONNECTED) 7] delete /laogong
[zk: localhost:2181(CONNECTED) 8] ls /
[zookeeper]
[zk: localhost:2181(CONNECTED) 9] create /laogong "laogong"
Created /laogong
[zk: localhost:2181(CONNECTED) 10] ls /
[laogong, zookeeper]
[zk: localhost:2181(CONNECTED) 11] get /laogong
laogong
[zk: localhost:2181(CONNECTED) 12] create /laogong/laopo "laopo"
Created /laogong/laopo
[zk: localhost:2181(CONNECTED) 13] ls /
[laogong, zookeeper]
[zk: localhost:2181(CONNECTED) 14] ls /laogong
[laopo]
[zk: localhost:2181(CONNECTED) 15] get /laogong/laopo
laopo
[zk: localhost:2181(CONNECTED) 16] 
```

上面的操作我都是在 `zknode01` 上面连接zk进行操作的，来看一下，在其他zkserver上有没有同步过来数据：


![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5123fa4105e74a04b3bc03a64b0f1566~tplv-k3u1fbpfcp-watermark.image?)

发现数据已经同步，`zknode03` 和 `zknode04` 数据也同步了。

再来看一下连接 `zknode02` 的连接状态：

```
[root@zknode02 ~]# netstat -natp   |   egrep  '(2888|3888)' 
tcp6       0      0 192.168.242.12:3888     :::*                    LISTEN      9530/java           
tcp6       0      0 192.168.242.12:3888     192.168.242.13:47474    ESTABLISHED 9530/java           
tcp6       0      0 192.168.242.12:37804    192.168.242.13:2888     ESTABLISHED 9530/java           
tcp6       0      0 192.168.242.12:3888     192.168.242.14:47530    ESTABLISHED 9530/java           
tcp6       0      0 192.168.242.12:39666    192.168.242.11:3888     ESTABLISHED 9530/java
```


![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5d60714faaca4ae79b2682057a67de9b~tplv-k3u1fbpfcp-watermark.image?)

连接状态分析：


![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4b4a86b5daa94bf79e9f5d4ad425304b~tplv-k3u1fbpfcp-watermark.image?)

上图是从 `zknode02` 服务器查看的，通过查看每台服务器，最终，zk集群的服务器每台都 **互相通信** 。

这个 `3888` 端口就是选举master用的，而 `2888` 端口是leader接受write请求用的。

## zk集群master选举

前面演示了有 4 个服务器的 **zk集群** ，其中 `zknode03` 是 leader 。

现在我把 `zknode03` 服务干掉：

```sh
^C[root@zknode03 conf]# zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Error contacting service. It is probably not running.
[root@zknode03 conf]# 

[root@localhost ~]# 
```

再来分别看一下 `zknode01` ~ `zknode04`的 zk server 状态：


![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e3716cab624341178b68be99f4f0b13d~tplv-k3u1fbpfcp-watermark.image?)


![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/eb5308df98a2490c9e9496a7031af60d~tplv-k3u1fbpfcp-watermark.image?)


![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c4a562a6ee9f41059f5ba4ce15825868~tplv-k3u1fbpfcp-watermark.image?)

可以看到 `zknode04` 自动成了 leader ！

事实上，**zk集群** 选举 leader 采用的是 `谦让` 的办法，谁的 id 大，选举谁。

那么前面为什么zknode3是leader呢？

因为我启动的顺序是 `zknode01` ~ `zknode04` 启动的，当 `zknode03` 的zk server 启动的时候，已经 **满足集群的最少节点数** 了，而且 `zknode03` 的 id 是 `当时` 最大的，所以 `zknode03` 的 server自动成了 leader 。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a14f1746505b4e4c98825a17ac5fe553~tplv-k3u1fbpfcp-zoom-1.image)

点个赞再走吧~
